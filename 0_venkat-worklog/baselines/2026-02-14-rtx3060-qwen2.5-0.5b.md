# Baseline Record: 2026-02-14 (RTX 3060, Qwen2.5-0.5B)

Status: reference baseline  
Scope: local machine, single GPU, sanitized log

## Environment

- GPU: NVIDIA GeForce RTX 3060 (12 GB)
- Driver: 570.133.07
- CUDA runtime (reported by `nvidia-smi`): 12.8
- Python: 3.12.12 (`uv` virtualenv)
- Key packages:
  - `torch==2.9.1+cu128`
  - `transformers==4.57.3`
  - `flashinfer==0.5.3`
  - `sgl-kernel==0.3.21`
  - `minisgl==0.1.0`

## Model

- `Qwen/Qwen2.5-0.5B-Instruct`

## Offline Baseline (`LLM` path)

Command flow: see `../RUNBOOK.md` section "One-Command Harness (Preferred)".

Observed:

- `init_time_s = 1.93`
- `warmup_time_s = 0.09` (`warmup_tokens = 15`)
- `single_req_time_s = 0.24` (`single_out_tokens = 63`)
- Batch run:
  - `batch_num_reqs = 16`
  - `batch_total_out_tokens = 1008`
  - `batch_time_s = 0.3778`
  - `batch_out_tok_per_s = 2667`

## Online Baseline (server + API path)

Server and client commands: see `../RUNBOOK.md` section "One-Command Harness (Preferred)".

Observed:

- Warmup:
  - `warmup_elapsed_s = 0.1091`
  - `tokens = 16`
- Benchmark (`batch_size = 8`, output length 64 each):
  - `bench_wall_time_s = 0.3296`
  - `Num requests = 8`
  - `Num tokens = 520`
  - `TTFT avg = 50.20 ms`
  - `TPOT avg = 4.4354 ms`
  - `E2E avg = 0.3296 s`
  - `Duration = 0.3315 s`
  - `Throughput = 1568 token/s`
  - `Req/s = 24.13`

## Usage

Use these numbers as baseline gates for Rust CPU-side changes:

- Parity first (no correctness regressions).
- Then performance target:
  - initial: `>= +10%` on targeted hot paths
  - milestone: `+20% to +30%` system throughput vs this baseline.

## CPU Backend A/B Check (2026-02-14, same machine/profile)

Compared `MINISGL_CPU_BACKEND=python` vs `MINISGL_CPU_BACKEND=rust_hotpath` using the same harness profile.

- Offline (`python -m minisgl.benchmark.harness offline`):
  - Python backend: `2672.64 tok/s` (`42.42 req/s`)
  - Rust backend: `2669.56 tok/s` (`42.37 req/s`)
  - Delta (Rust vs Python): `-0.12%`
- Online (`python -m minisgl.benchmark.harness online` with local server):
  - Python backend: `1607.44 tok/s` (`24.73 req/s`), `TTFT 48.76 ms`, `TPOT 4.3326 ms`
  - Rust backend: `1570.27 tok/s` (`24.16 req/s`), `TTFT 49.29 ms`, `TPOT 4.4329 ms`
  - Delta (Rust vs Python): `-2.31%`

Isolated metadata microbench (CPU-only, scheduler builders loop):

- Python backend: `6547.08 ops/s` (`0.1527 ms/op`)
- Rust backend: `1789.07 ops/s` (`0.5590 ms/op`)

Key diagnosis:

- FFI boundary overhead is still dominant in current in-process binding path.
- Rust core logic is fast, but Python list/tensor marshaling can erase gains.

## CPU Backend A/B Check (Iteration 2, packed buffers)

After packed-buffer binding update (`make_metadata_buffers`) and one-call-per-step metadata generation:

- Offline:
  - Python backend: `2664.56 tok/s`
  - Rust backend: `2664.09 tok/s`
  - Delta: `-0.02%` (near parity)
- Online:
  - Python backend: `1590.42 tok/s`, `TTFT 48.89 ms`, `TPOT 4.3747 ms`
  - Rust backend: `1614.35 tok/s`, `TTFT 48.81 ms`, `TPOT 4.3125 ms`
  - Delta: `+1.50%` (Rust ahead)

Scheduler metadata microbench (CPU-only, same synthetic batch):

- Python backend: `6428.18 ops/s`
- Rust backend: `10508.27 ops/s`

## CPU Backend A/B Check (Iteration 3, shadow tooling added)

After adding shadow-mode comparator/reporting (disabled in normal path), re-ran baseline A/B:

- Offline:
  - Python backend: `2676.06 tok/s`
  - Rust backend: `2677.83 tok/s`
  - Delta: `+0.07%` (Rust ahead, near parity)
- Online:
  - Python backend: `1582.59 tok/s`
  - Rust backend: `1593.44 tok/s`
  - Delta: `+0.69%` (Rust ahead)

Shadow-mode parity check (offline harness with `MINISGL_CPU_BACKEND_SHADOW=1`):

- `divergence_entries=0` on this deterministic run profile.

## CPU Backend A/B Check (Iteration 4, deterministic token parity)

Added deterministic token parity command to validate exact output token IDs across backends:

- Command:
  - `python -m minisgl.benchmark.token_parity --model-path Qwen/Qwen2.5-0.5B-Instruct --backend-a python --backend-b rust_hotpath --max-tokens 16 --token-prompt-count 4 --min-input-len 32 --max-input-len 64 --cuda-graph-max-bs 1 --master-port 2380 --out 0_venkat-worklog/baselines/latest-token-parity.json`
- Result:
  - `parity_passed=True`
  - `text_prompts`: `0` mismatches
  - `token_prompts`: `0` mismatches
  - `text_prompts` signature: `71b1f353c3d12c2563754c0fbf9125d57ea39d2f7b702bc1e7fbf66596fe484a`
  - `token_prompts` signature: `c85bc50cb9ee78e4430a9cc24725b4c17073cfd689b3f71bafb3f1f378d34e2b`

## CPU Backend A/B Check (Iteration 5, expanded parity corpus + low-overhead shadow)

Deterministic token parity corpus now includes:

- short prompts: `latest-token-parity-short.json`
- long prompts: `latest-token-parity-long.json`
- shared-prefix heavy prompts: `latest-token-parity-shared-prefix.json`

All three runs report:

- `parity_passed=True`
- `text_prompts` mismatches: `0`
- `token_prompts` mismatches: `0`

Shadow mode overhead control:

- Added env knob: `MINISGL_CPU_BACKEND_SHADOW_EVERY_N` (default `1`) to sample shadow comparisons every N metadata calls for CI/continuous runs.
- Deterministic shadow check (`latest-offline-shadow-check.json`) observed: `divergence_entries=0`.
- Mixed-sampling shadow check observed: `divergence_entries=0`.

See full corpus record: `0_venkat-worklog/baselines/2026-02-14-shadow-parity-corpus.md`.
