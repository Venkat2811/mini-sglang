# Baseline Record: 2026-02-14 (RTX 3060, Qwen2.5-0.5B)

Status: reference baseline  
Scope: local machine, single GPU, sanitized log

## Environment

- GPU: NVIDIA GeForce RTX 3060 (12 GB)
- Driver: 570.133.07
- CUDA runtime (reported by `nvidia-smi`): 12.8
- Python: 3.12.12 (`uv` virtualenv)
- Key packages:
  - `torch==2.9.1+cu128`
  - `transformers==4.57.3`
  - `flashinfer==0.5.3`
  - `sgl-kernel==0.3.21`
  - `minisgl==0.1.0`

## Model

- `Qwen/Qwen2.5-0.5B-Instruct`

## Offline Baseline (`LLM` path)

Command flow: see `../RUNBOOK.md` section "One-Command Harness (Preferred)".

Observed:

- `init_time_s = 1.93`
- `warmup_time_s = 0.09` (`warmup_tokens = 15`)
- `single_req_time_s = 0.24` (`single_out_tokens = 63`)
- Batch run:
  - `batch_num_reqs = 16`
  - `batch_total_out_tokens = 1008`
  - `batch_time_s = 0.3778`
  - `batch_out_tok_per_s = 2667`

## Online Baseline (server + API path)

Server and client commands: see `../RUNBOOK.md` section "One-Command Harness (Preferred)".

Observed:

- Warmup:
  - `warmup_elapsed_s = 0.1091`
  - `tokens = 16`
- Benchmark (`batch_size = 8`, output length 64 each):
  - `bench_wall_time_s = 0.3296`
  - `Num requests = 8`
  - `Num tokens = 520`
  - `TTFT avg = 50.20 ms`
  - `TPOT avg = 4.4354 ms`
  - `E2E avg = 0.3296 s`
  - `Duration = 0.3315 s`
  - `Throughput = 1568 token/s`
  - `Req/s = 24.13`

## Usage

Use these numbers as baseline gates for Rust CPU-side changes:

- Parity first (no correctness regressions).
- Then performance target:
  - initial: `>= +10%` on targeted hot paths
  - milestone: `+20% to +30%` system throughput vs this baseline.

## CPU Backend A/B Check (2026-02-14, same machine/profile)

Compared `MINISGL_CPU_BACKEND=python` vs `MINISGL_CPU_BACKEND=rust_hotpath` using the same harness profile.

- Offline (`python -m minisgl.benchmark.harness offline`):
  - Python backend: `2672.64 tok/s` (`42.42 req/s`)
  - Rust backend: `2669.56 tok/s` (`42.37 req/s`)
  - Delta (Rust vs Python): `-0.12%`
- Online (`python -m minisgl.benchmark.harness online` with local server):
  - Python backend: `1607.44 tok/s` (`24.73 req/s`), `TTFT 48.76 ms`, `TPOT 4.3326 ms`
  - Rust backend: `1570.27 tok/s` (`24.16 req/s`), `TTFT 49.29 ms`, `TPOT 4.4329 ms`
  - Delta (Rust vs Python): `-2.31%`

Isolated metadata microbench (CPU-only, scheduler builders loop):

- Python backend: `6547.08 ops/s` (`0.1527 ms/op`)
- Rust backend: `1789.07 ops/s` (`0.5590 ms/op`)

Key diagnosis:

- FFI boundary overhead is still dominant in current in-process binding path.
- Rust core logic is fast, but Python list/tensor marshaling can erase gains.

## CPU Backend A/B Check (Iteration 2, packed buffers)

After packed-buffer binding update (`make_metadata_buffers`) and one-call-per-step metadata generation:

- Offline:
  - Python backend: `2664.56 tok/s`
  - Rust backend: `2664.09 tok/s`
  - Delta: `-0.02%` (near parity)
- Online:
  - Python backend: `1590.42 tok/s`, `TTFT 48.89 ms`, `TPOT 4.3747 ms`
  - Rust backend: `1614.35 tok/s`, `TTFT 48.81 ms`, `TPOT 4.3125 ms`
  - Delta: `+1.50%` (Rust ahead)

Scheduler metadata microbench (CPU-only, same synthetic batch):

- Python backend: `6428.18 ops/s`
- Rust backend: `10508.27 ops/s`
